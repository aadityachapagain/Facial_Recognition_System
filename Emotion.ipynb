{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from  matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fer2013/fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      "emotion    35887 non-null int64\n",
      "pixels     35887 non-null object\n",
      "Usage      35887 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = np.array([int(i) for i in df['pixels'][0].split(' ')]).reshape((48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x17E0FD3E940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = pic.astype(np.uint8)\n",
    "Image.fromarray(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x17E0FDD0630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_image = np.fromstring(str(df['pixels'][0]), dtype=np.uint8, sep=' ').reshape((48, 48))\n",
    "Image.fromarray(data_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_FACE = 48\n",
    "EMOTIONS = ['angry', 'disgusted', 'fearful',\n",
    "            'happy', 'sad', 'surprised', 'neutral']\n",
    "SAVE_DIRECTORY = 'emo_data'\n",
    "SAVE_MODEL_FILENAME = 'Gudi_model_100_epochs_20000_faces'\n",
    "DATASET_CSV_FILENAME = 'fer2013.csv'\n",
    "SAVE_DATASET_IMAGES_FILENAME = 'data_images.npy'\n",
    "SAVE_DATASET_LABELS_FILENAME = 'data_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70,  80,  82, ...,  52,  43,  41],\n",
       "       [ 65,  61,  58, ...,  56,  52,  44],\n",
       "       [ 50,  43,  54, ...,  49,  56,  47],\n",
       "       ...,\n",
       "       [ 91,  65,  42, ...,  72,  56,  43],\n",
       "       [ 77,  82,  79, ..., 105,  70,  46],\n",
       "       [ 77,  72,  84, ..., 106, 109,  82]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(image):\n",
    "    if len(image.shape) > 2 and image.shape[2] == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        image = cv2.imdecode(image, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "    gray_border = np.zeros((150, 150), np.uint8)\n",
    "    gray_border[:, :] = 200\n",
    "    gray_border[\n",
    "        int((150 / 2) - (SIZE_FACE / 2)): int((150 / 2) + (SIZE_FACE / 2)),\n",
    "        int((150 / 2) - (SIZE_FACE / 2)): int((150 / 2) + (SIZE_FACE / 2))\n",
    "    ] = image\n",
    "    image = gray_border\n",
    "    faces = cascade_classifier.detectMultiScale(\n",
    "        image,\n",
    "        scaleFactor=1.3,\n",
    "        minNeighbors=5\n",
    "    )\n",
    "    #  None is we don't found an image\n",
    "    if not len(faces) > 0:\n",
    "        return None\n",
    "    max_area_face = faces[0]\n",
    "    for face in faces:\n",
    "        if face[2] * face[3] > max_area_face[2] * max_area_face[3]:\n",
    "            max_area_face = face\n",
    "    # Chop image to face\n",
    "    face = max_area_face\n",
    "    image = image[face[1]:(face[1] + face[2]), face[0]:(face[0] + face[3])]\n",
    "    # Resize image to network size\n",
    "\n",
    "    try:\n",
    "        image = cv2.resize(image, (SIZE_FACE, SIZE_FACE),\n",
    "                           interpolation=cv2.INTER_CUBIC) / 255.\n",
    "    except Exception:\n",
    "        print(\"[+] Problem during resize\")\n",
    "        return None\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_vec(x):\n",
    "    d = np.zeros(len(EMOTIONS))\n",
    "    d[x] = 1.0\n",
    "    return d\n",
    "\n",
    "\n",
    "def flip_image(image):\n",
    "    return cv2.flip(image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_image(data):\n",
    "    data_image = np.fromstring(str(data), dtype=np.uint8, sep=' ').reshape((SIZE_FACE, SIZE_FACE))\n",
    "    data_image = Image.fromarray(data_image).convert('RGB')\n",
    "    data_image = np.array(data_image)[:, :, ::-1].copy()\n",
    "    data_image = format_image(data_image)\n",
    "    return data_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df\n",
    "\n",
    "labels = []\n",
    "images = []\n",
    "index = 1\n",
    "total = data.shape[0]\n",
    "for index, row in data.iterrows():\n",
    "    emotion = emotion_to_vec(row['emotion'])\n",
    "    image = data_to_image(row['pixels'])\n",
    "    if image is not None:\n",
    "        labels.append(emotion)\n",
    "        images.append(image)\n",
    "        # images.append(flip_image(image))\n",
    "    index += 1\n",
    "    print(\"Progress: {}/{} {:.2f}%\".format(index, total, index * 100.0 / total))\n",
    "\n",
    "print(\"Total: \" + str(len(images)))\n",
    "np.save(os.path.join(SAVE_DIRECTORY, SAVE_DATASET_IMAGES_FILENAME), images)\n",
    "np.save(os.path.join(SAVE_DIRECTORY, SAVE_DATASET_LABELS_FILENAME), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load(os.path.join(SAVE_DIRECTORY, SAVE_DATASET_IMAGES_FILENAME))\n",
    "images = images.reshape([-1, SIZE_FACE, SIZE_FACE, 1])\n",
    "labels = np.load(os.path.join(SAVE_DIRECTORY, SAVE_DATASET_LABELS_FILENAME)).reshape([-1, len(EMOTIONS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14018"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14018, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(images, labels, test_size=0.18, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11494, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W', dtype=tf.float32, shape=shape, initializer=initer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b', dtype=tf.float32, initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, num_filters, filter_size, stride, name):\n",
    "    \"\"\"\n",
    "    Create a 2D convolution layer\n",
    "    :param x: input from previous layer\n",
    "    :param filter_size: size of each filter\n",
    "    :param num_filters: number of filters (or output feature maps)\n",
    "    :param stride: filter stride\n",
    "    :param name: layer name\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        num_in_channel = x.get_shape().as_list()[-1]\n",
    "        shape = [filter_size, filter_size, num_in_channel, num_filters]\n",
    "        W = weight_variable(shape=shape)\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_filters])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "        layer += b\n",
    "#         Using non - linearity RELU\n",
    "        return tf.nn.relu(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(x, ksize, stride, name):\n",
    "    \"\"\"\n",
    "    Create a max pooling layer\n",
    "    :param x: input to max-pooling layer\n",
    "    :param ksize: size of the max-pooling filter\n",
    "    :param stride: stride of the max-pooling filter\n",
    "    :param name: layer name\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x,\n",
    "                          ksize=[1, ksize, ksize, 1],\n",
    "                          strides=[1, stride, stride, 1],\n",
    "                          padding=\"SAME\",\n",
    "                          name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    \"\"\"\n",
    "    Flattens the output of the convolutional layer to be fed into fully-connected layer\n",
    "    :param layer: input array\n",
    "    :return: flattened array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Flatten_layer'):\n",
    "        layer_shape = layer.get_shape()\n",
    "        num_features = layer_shape[1:4].num_elements()\n",
    "        layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_out: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        in_dim = x.get_shape()[1]\n",
    "        W = weight_variable(shape=[in_dim, num_units])\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_units])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.matmul(x, W)\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, keep_prob,name):\n",
    "    \"\"\"Create a dropout layer.\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :, :, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001  # The optimization initial learning rate\n",
    "epochs = 20  # Total number of training epochs\n",
    "batch_size = 50  # Training batch size\n",
    "display_freq = 50  # Frequency of displaying the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, SIZE_FACE, SIZE_FACE, 1], name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 7], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv_layer(x, 64, 5,stride=1, name='conv1')\n",
    "pool1 = max_pool(conv1, 3, stride=2, name='pool1')\n",
    "conv2 = conv_layer(pool1,64, 5, stride=1,name='conv2')\n",
    "pool2 = max_pool(conv2, 3, stride=2, name='pool2')\n",
    "\n",
    "conv3 = conv_layer(pool2, 128, 4, stride=1, name='conv3')\n",
    "drop = dropout(conv3, 0.3,name='drop')\n",
    "\n",
    "layer_flat = flatten_layer(drop)\n",
    "fc1 = fc_layer(layer_flat,3072,use_relu=True,name='fc1')\n",
    "output_logits= fc_layer(fc1, len(EMOTIONS) ,name='OUT', use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Train'):\n",
    "    with tf.variable_scope('Loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output_logits), name='loss')\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam-op').minimize(loss)\n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    with tf.variable_scope('Prediction'):\n",
    "        cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Merge all summaries\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11494, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Loss=1.94,\tTraining Accuracy=34.0%\n",
      "iter  50:\t Loss=1.83,\tTraining Accuracy=22.0%\n",
      "iter 100:\t Loss=1.86,\tTraining Accuracy=24.0%\n",
      "iter 150:\t Loss=1.78,\tTraining Accuracy=28.0%\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "global_step = 0\n",
    "summary_writer = tf.summary.FileWriter('./logdir', sess.graph)\n",
    "# Number of training iterations in each epoch\n",
    "num_tr_iter = int(len(y_train) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    x_train, y_train = randomize(x_train, y_train)\n",
    "    for iteration in range(num_tr_iter):\n",
    "        global_step += 1\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % display_freq == 0:\n",
    "            # Calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch, summary_tr = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict=feed_dict_batch)\n",
    "            summary_writer.add_summary(summary_tr, global_step)\n",
    "\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # Run validation after every epoch\n",
    "    feed_dict_valid = {x: x_valid, y: y_valid}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Alex net for Emotion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognition:\n",
    "    def build_network(self):\n",
    "        print('[+] Building CNN')\n",
    "        self.network = input_data(shape=[None, SIZE_FACE, SIZE_FACE, 1])\n",
    "        \n",
    "        self.network = conv_2d(self.network, 64, 5, activation='relu')\n",
    "        #self.network = local_response_normalization(self.network)\n",
    "        self.network = max_pool_2d(self.network, 3, strides=2)\n",
    "        self.network = conv_2d(self.network, 64, 5, activation='relu')\n",
    "        self.network = max_pool_2d(self.network, 3, strides=2)\n",
    "        \n",
    "        \n",
    "        self.network = conv_2d(self.network, 128, 4, activation='relu')\n",
    "        self.network = dropout(self.network, 0.3)\n",
    "        self.network = fully_connected(self.network, 3072, activation='relu')\n",
    "        self.network = fully_connected(\n",
    "            self.network, len(EMOTIONS), activation='softmax')\n",
    "        self.network = regression(\n",
    "            self.network,\n",
    "            optimizer='momentum',\n",
    "            loss='categorical_crossentropy'\n",
    "        )\n",
    "        print('creating model ......')\n",
    "        self.model = tflearn.DNN(\n",
    "            self.network,\n",
    "            checkpoint_path=SAVE_DIRECTORY + '/emotion_recognition',\n",
    "            max_checkpoints=1,\n",
    "            tensorboard_verbose=2\n",
    "        )\n",
    "        print('loading model .....')\n",
    "        self.load_model()\n",
    "\n",
    "        \n",
    "    def start_training(self):\n",
    "        self.build_network()\n",
    "        # Training\n",
    "        print(' Training network')\n",
    "        self.model.fit(\n",
    "            images, labels,\n",
    "            validation_set=(images_test,labels_test),\n",
    "            n_epoch=10,\n",
    "            batch_size=50,\n",
    "            shuffle=True,\n",
    "            show_metric=True,\n",
    "            snapshot_step=200,\n",
    "            snapshot_epoch=True,\n",
    "            run_id='emotion_recognition'\n",
    "        )\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, image):\n",
    "        if image is None:\n",
    "            return None\n",
    "        image = image.reshape([-1, SIZE_FACE, SIZE_FACE, 1])\n",
    "        return self.model.predict(image)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save(os.path.join(SAVE_DIRECTORY, SAVE_MODEL_FILENAME))\n",
    "        print('[+] Model trained and saved at ' + SAVE_MODEL_FILENAME)\n",
    "\n",
    "    def load_model(self):\n",
    "        print(os.path.join(os.getcwd(),SAVE_DIRECTORY, SAVE_MODEL_FILENAME))\n",
    "        if os.path.isfile(os.path.join(os.getcwd(),SAVE_DIRECTORY, SAVE_MODEL_FILENAME)):\n",
    "            self.model.load(os.path.join(SAVE_DIRECTORY, SAVE_MODEL_FILENAME),weights_only=True)\n",
    "            print('[+] Model loaded from ' + SAVE_MODEL_FILENAME)\n",
    "        else:\n",
    "            print('file not found ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod([48,48,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = EmotionRecognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
