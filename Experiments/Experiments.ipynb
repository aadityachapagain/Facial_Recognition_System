{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import iteritems, string_types\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_PIC_SIZE = 160\n",
    "\n",
    "EMBEDDING_SIZE = 512\n",
    "\n",
    "PRETREINED_MODEL_DIR = os.path.join(os.getcwd(),'pretrained_models')\n",
    "\n",
    "UNKNOWN_CLASS = \"Mr Nobody\"\n",
    "\n",
    "CLUSTER = \"_embeddings.KNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = ['SAME','VALID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_path, session, ignore_missing=False):\n",
    "    \"\"\"Load network weights.\n",
    "    @data_path: The path to the numpy-serialized network weights\n",
    "    @session: The current TensorFlow session\n",
    "    @ignore_missing: If true, serialized weights for missing layers are ignored.\n",
    "    \"\"\"\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()  # pylint: disable=no-member\n",
    "\n",
    "    for op_name in data_dict:\n",
    "        with tf.variable_scope(op_name, reuse=True):\n",
    "            for param_name, data in iteritems(data_dict[op_name]):\n",
    "                try:\n",
    "                    var = tf.get_variable(param_name)\n",
    "                    session.run(var.assign(data))\n",
    "                except ValueError:\n",
    "                    if not ignore_missing:\n",
    "                        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(inpt, k_h, k_w, c_o, s_h, s_w, name, relu=True, padding=padding[0], group=1, biased=True):\n",
    "    \"\"\"\n",
    "    @inpt: input data to convolve\n",
    "    @k_h: kernel height  \n",
    "    @k_w: kernel width\n",
    "    @c_o: number of filter\n",
    "    @s_h: strides weight\n",
    "    @s_w: strides height \n",
    "    @name: layer name\n",
    "    @relu: non linearity RELU\n",
    "    @padding: filter padding \n",
    "    \"\"\"\n",
    "# Get the number of channels in the input\n",
    "    c_i = int(inpt.get_shape()[-1])\n",
    "\n",
    "    # Convolution for a given input and kernel\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = tf.get_variable('weights', shape=[k_h, k_w, c_i // group, c_o], trainable=True)\n",
    "        output = tf.nn.conv2d(inpt, kernel, [1, s_h, s_w, 1], padding=padding)\n",
    "        # Add the biases\n",
    "        if biased:\n",
    "            biases = tf.get_variable('biases', [c_o])\n",
    "            output = tf.nn.bias_add(output, biases)\n",
    "        if relu:\n",
    "            # ReLU non-linearity\n",
    "            output = tf.nn.relu(output, name=scope.name)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prelu(inpt, name):\n",
    "    \"\"\"\n",
    "    @inpt: input from previous layer\n",
    "    @name: layer name\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        i = int(inpt.get_shape()[-1])\n",
    "        alpha = tf.get_variable('alpha', shape=(i,))\n",
    "        output = tf.nn.relu(inpt) + tf.multiply(alpha, -tf.nn.relu(-inpt))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(inpt, k_h, k_w, s_h, s_w, name, padding=padding[0]):\n",
    "    \"\"\"\n",
    "    @inpt: input from previous layer\n",
    "    @k_h: kernel height\n",
    "    @k_w: kernel width\n",
    "    @s_h: stride height\n",
    "    @s_w: stride width\n",
    "    @name: layer name\n",
    "    @padding: Valid Padding ['SAME','VALID']\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(inpt, ksize=[1, k_h, k_w, 1],strides=[1, s_h, s_w, 1], padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(inpt, num_out, name, relu=True):\n",
    "    \"\"\"\n",
    "    @inpt: input from previous layer\n",
    "    @num_out: output layer\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = inpt.get_shape()\n",
    "        if input_shape.ndims == 4:\n",
    "            # since the input is spatial.we have to Vectorize it first.\n",
    "            dim = 1\n",
    "            for d in input_shape[1:].as_list():\n",
    "                dim *= int(d)\n",
    "            feed_in = tf.reshape(inpt, [-1, dim])\n",
    "        else:\n",
    "            feed_in, dim = (inpt, input_shape[-1].value)\n",
    "        weights = tf.get_variable('weights', shape=[dim, num_out])\n",
    "        biases = tf.get_variable('biases', [num_out])\n",
    "        op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n",
    "        fc = op(feed_in, weights, biases, name=name)\n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(target, axis, name=None):\n",
    "    \"\"\"\n",
    "    @target: target layer to apply softmax\n",
    "    @axis: axes to apply softmax\n",
    "    @name: layer name\n",
    "    \"\"\"\n",
    "    max_axis = tf.reduce_max(target, axis, keepdims=True)\n",
    "    target_exp = tf.exp(target - max_axis)\n",
    "    normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n",
    "    softmax = tf.div(target_exp, normalize, name)\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pNet(data):\n",
    "    value = conv(data, 3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')\n",
    "    value = prelu(value, name='PReLU1')\n",
    "    value = max_pool(value, 2, 2, 2, 2, name='pool1')\n",
    "    value = conv(value ,3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')\n",
    "    value = prelu(value ,name='PReLU2')\n",
    "    value = conv(value ,3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')\n",
    "    value = prelu(value ,name='PReLU3')\n",
    "#     Face detection classfication probablity output\n",
    "    val_conv4_1 = conv(value ,1, 1, 2, 1, 1, relu=False, name='conv4-1')\n",
    "    classify = softmax(val_conv4_1 ,3, name='prob1')\n",
    "#     bounding box regression\n",
    "    bbr = conv(value ,1, 1, 4, 1, 1, relu=False, name='conv4-2')\n",
    "    \n",
    "    return(classify, bbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rNet(data):\n",
    "    value = conv(data, 3, 3, 28, 1, 1, padding='VALID', relu=False, name='conv1')\n",
    "    value = prelu(value, name='prelu1')\n",
    "    value = max_pool(value, 3, 3, 2, 2, name='pool1')\n",
    "    value = conv(value, 3, 3, 48, 1, 1, padding='VALID', relu=False, name='conv2')\n",
    "    value = prelu(value, name='prelu2')\n",
    "    value = max_pool(value, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "    value = conv(value, 2, 2, 64, 1, 1, padding='VALID', relu=False, name='conv3')\n",
    "    value = prelu(value, name='prelu3')\n",
    "    value = fully_connected(value, 128, relu=False, name='conv4')\n",
    "    value = prelu(value, name='prelu4')\n",
    "#     face detection classfication probablity output\n",
    "    conv5_1 = fully_connected(value, 2, relu=False, name='conv5-1')\n",
    "    classify = softmax(conv5_1, 1, name='prob1')\n",
    "#     bounding box regression\n",
    "    bbr = fully_connected(value,4, relu=False, name='conv5-2')\n",
    "    \n",
    "    return (classify, bbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oNet(data):\n",
    "    value = conv(data, 3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv1')\n",
    "    value = prelu(value, name='prelu1')\n",
    "    value = max_pool(value ,3, 3, 2, 2, name='pool1')\n",
    "    value = conv(value, 3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv2')\n",
    "    value = prelu(value, name='prelu2')\n",
    "    value = max_pool(value, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "    value = conv(value, 3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv3')\n",
    "    value = prelu(value, name='prelu3')\n",
    "    value = max_pool(value, 2, 2, 2, 2, name='pool3')\n",
    "    value = conv(value, 2, 2, 128, 1, 1, padding='VALID', relu=False, name='conv4')\n",
    "    value = prelu(value, name='prelu4')\n",
    "    value = fully_connected(value, 256, relu=False, name='conv5')\n",
    "    value = prelu(value, name='prelu5')\n",
    "#     face detection classification\n",
    "    conv6_1 = fully_connected(value, 2, relu=False, name='conv6-1')\n",
    "    classify = softmax(conv6_1, 1, name='prob1')\n",
    "#     bounding box regression\n",
    "    bbr = fully_connected(value, 4, relu=False, name='conv6-2')\n",
    "#     face localization\n",
    "    floc = fully_connected(value, 10, relu=False, name='conv6-3')\n",
    "    \n",
    "    return (classify, bbr, floc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mtcnn(sess, model_path):\n",
    "    \"\"\"\n",
    "    Input @sess: Current session,\n",
    "    Input @model_path: path of weights of model to load,\n",
    "    \"\"\"\n",
    "#     Creating variable scope for Pnet\n",
    "    with tf.variable_scope('pnet'):\n",
    "        data = tf.placeholder(tf.float32, (None, None, None, 3), 'input')\n",
    "        pnet = pNet(data)\n",
    "        load(os.path.join(model_path, 'det1.npy'), sess)\n",
    "        \n",
    "#     Creating Variabel scope for Rnet\n",
    "    with tf.variable_scope('rnet'):\n",
    "        data = tf.placeholder(tf.float32, (None, 24, 24, 3), 'input')\n",
    "        rnet = rNet(data)\n",
    "        load(os.path.join(model_path, 'det2.npy'), sess)\n",
    "        \n",
    "#    Creating Variable scope for Onet\n",
    "    with tf.variable_scope('onet'):\n",
    "        data = tf.placeholder(tf.float32, (None, 48, 48, 3), 'input')\n",
    "        onet = oNet(data)\n",
    "        load(os.path.join(model_path, 'det3.npy'), sess)\n",
    "        \n",
    "#         Predicting facial features by cascading P-net, R-net and O-net\n",
    "    pnet_fun = lambda img: sess.run(('pnet/conv4-2/BiasAdd:0', 'pnet/prob1:0'), feed_dict={'pnet/input:0': img})\n",
    "    rnet_fun = lambda img: sess.run(('rnet/conv5-2/conv5-2:0', 'rnet/prob1:0'), feed_dict={'rnet/input:0': img})\n",
    "    onet_fun = lambda img: sess.run(('onet/conv6-2/conv6-2:0', 'onet/conv6-3/conv6-3:0', 'onet/prob1:0'), feed_dict={'onet/input:0': img})\n",
    "    \n",
    "    return pnet_fun, rnet_fun, onet_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setup_mtcnn():\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            return create_mtcnn(sess, PRETREINED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\MLTUT\\\\Experiments\\\\pretrained_models\\\\det1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0553470ecae3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_setup_mtcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-a56aabb58923>\u001b[0m in \u001b[0;36m_setup_mtcnn\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_mtcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPRETREINED_MODEL_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-dec0d861098a>\u001b[0m in \u001b[0;36mcreate_mtcnn\u001b[1;34m(sess, model_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'det1.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     Creating Variabel scope for Rnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fdb791a1d026>\u001b[0m in \u001b[0;36mload\u001b[1;34m(data_path, session, ignore_missing)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_missing\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserialized\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0mare\u001b[0m \u001b[0mignored\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=no-member\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mop_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\welcome\\envs\\mltut\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\MLTUT\\\\Experiments\\\\pretrained_models\\\\det1.npy'"
     ]
    }
   ],
   "source": [
    "pnet, rnet, onet = _setup_mtcnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
